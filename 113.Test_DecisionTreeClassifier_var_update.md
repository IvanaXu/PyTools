> https://github.com/IvanaXu/PyTools/blob/master/013.Test_DecisionTreeClassifier_var.ipynb

```shell
#
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree.export import export_text

N, M, max_depth = 100, 10000, 3
Cols = [
    f"X{icol}"
    for icol in range(N)
]
_x, _y = make_classification(
    n_samples=M, 
    n_features=N, 
    n_informative=N//3,
)
data = pd.DataFrame(_x, columns=Cols)
data["Target"] = _y

X = data[Cols]
y = data["Target"]
print(X.head())
print(pd.value_counts(y))

model = DecisionTreeClassifier(
    max_depth=max_depth,
    min_samples_leaf=M//20,
    random_state=10086,
)
model.fit(X, y)
print(f"Score: {model.score(X, y):f}")

# 
_e = export_text(
    model, feature_names=Cols, show_weights=True, decimals=4)

_rsep0, _rsep1 = "|   ", "|---"
_rule, i_rule, j_rule = [], [np.nan for _depth in range(max_depth)], None
for i_e in _e.split("\n"):
    print(i_e)
    if not i_e:
        continue
    elif "weights" in i_e:
        _rule.append(j_rule)
        continue

    _rule0 = f"""{i_e.split(_rsep1)[0]}{_rsep1}"""
    _rule1 = i_e.split(_rsep1)[1].strip(" ")

    for _depth in range(max_depth):
        _leaf = f"""{_rsep0*_depth}{_rsep1}"""
        if _rule0 == _leaf:
            if _depth == 0:
                i_rule = [np.nan for _depth in range(max_depth)]
            i_rule[_depth] = _rule1
            j_rule = i_rule.copy()

            if _depth + 1 == max_depth:
                i_rule[_depth] = np.nan
            break

_rule_cols = [f"RULE{_depth}" for _depth in range(max_depth)]
_vars = [icol for icol, _0 in zip(X.columns, model.feature_importances_) if _0 > 0]
_rule = pd.DataFrame(_rule, columns=_rule_cols)
# print(_rule)

assert _rule.shape[0] <= 2**max_depth
assert _rule.shape == _rule.drop_duplicates().shape

_result = []
for irule in _rule.to_dict("split")["data"]:
    idata = data.copy()
    for jrule in irule:
        if not pd.isna(jrule):
            _r0 = jrule.split(" ")[0]
            _r1 = jrule.replace(_r0, " ")
            idata = eval(f"""idata[idata["{_r0}"]{_r1}]""")
    _result.append(irule+[idata])
_result = pd.DataFrame(_result, columns=_rule_cols+["idata"])
# print(_result)

_result["cnt0"] = _result["idata"].apply(lambda x: x[x["Target"] == 0].shape[0])
_result["cnt1"] = _result["idata"].apply(lambda x: x[x["Target"] == 1].shape[0])
assert _result["cnt0"].sum() == data[data["Target"] == 0].shape[0]
assert _result["cnt1"].sum() == data[data["Target"] == 1].shape[0]

_result["pcnt0"] = _result["cnt0"]/_result["cnt0"].sum()
_result["pcnt1"] = _result["cnt1"]/_result["cnt1"].sum()

_result["rate0"] = _result["cnt0"]/(_result["cnt0"]+_result["cnt1"])
_result["rate1"] = _result["cnt1"]/(_result["cnt0"]+_result["cnt1"])
_result["Arate1"] = _result["cnt1"].sum()/(_result["cnt0"].sum()+_result["cnt1"].sum())
_result["lift"] = _result["rate1"]/_result["Arate1"]

_result.sort_values("lift", ascending=False, inplace=True)
_result["iKS"] = _result["cnt1"].cumsum()/_result["cnt1"].sum() - _result["cnt0"].cumsum()/_result["cnt0"].sum()
_result["KS"] = KS = _result["iKS"].max()
print(f"KS: {KS:f}")

_result.drop(["idata", "iKS"], axis=1, inplace=True)
_result.sort_values(_rule_cols, ascending=True, inplace=True)
_result.to_csv("TreeRule.csv", index=False)

print(_result)

```
> 1    5009
> 
> 0    4991
> 
> Name: Target, dtype: int64
> 
> Score: 0.675900
> 
> |--- X60 <= 1.8027
> 
> |   |--- X29 <= 1.7300
> 
> |   |   |--- X90 <= 0.8090
> 
> |   |   |   |--- weights: [1298.0000, 1652.0000] class: 1
> 
> |   |   |--- X90 >  0.8090
> 
> |   |   |   |--- weights: [531.0000, 1879.0000] class: 1
> 
> |   |--- X29 >  1.7300
> 
> |   |   |--- X14 <= -0.1858
> 
> |   |   |   |--- weights: [421.0000, 487.0000] class: 1
> 
> |   |   |--- X14 >  -0.1858
> 
> |   |   |   |--- weights: [741.0000, 212.0000] class: 0
> 
> |--- X60 >  1.8027
> 
> |   |--- X39 <= 0.9091
> 
> |   |   |--- X90 <= 0.4820
> 
> |   |   |   |--- weights: [1203.0000, 200.0000] class: 0
> 
> |   |   |--- X90 >  0.4820
> 
> |   |   |   |--- weights: [463.0000, 265.0000] class: 0
> 
> |   |--- X39 >  0.9091
> 
> |   |   |--- weights: [334.0000, 314.0000] class: 0
> 
> KS: 0.351345
> 

| RULE0        | RULE1          | RULE2          | cnt0 | cnt1 | pcnt0       | pcnt1       | rate0       | rate1       | Arate1 | lift        | KS          |
| ------------ | -------------- | -------------- | ---- | ---- | ----------- | ----------- | ----------- | ----------- | ------ | ----------- | ----------- |
| X6 <= 0.0896 | X13 <= -1.6769 | X61 <= 0.2357  | 581  | 177  | 0.116502908 | 0.035308199 | 0.766490765 | 0.233509235 | 0.5013 | 0.46580737  | 0.408928924 |
| X6 <= 0.0896 | X13 <= -1.6769 | X61 > 0.2357   | 347  | 490  | 0.06958091  | 0.097745861 | 0.414575866 | 0.585424134 | 0.5013 | 1.167811957 | 0.408928924 |
| X6 <= 0.0896 | X13 > -1.6769  | X5 <= -0.3499  | 285  | 1444 | 0.057148586 | 0.288051067 | 0.164835165 | 0.835164835 | 0.5013 | 1.665998075 | 0.408928924 |
| X6 <= 0.0896 | X13 > -1.6769  | X5 > -0.3499   | 709  | 1119 | 0.142169641 | 0.223219629 | 0.38785558  | 0.61214442  | 0.5013 | 1.221113944 | 0.408928924 |
| X6 > 0.0896  | X61 <= -0.5179 | X13 <= 1.1055  | 1225 | 158  | 0.245638661 | 0.031518053 | 0.885755604 | 0.114244396 | 0.5013 | 0.227896262 | 0.408928924 |
| X6 > 0.0896  | X61 <= -0.5179 | X13 > 1.1055   | 477  | 324  | 0.095648687 | 0.064631957 | 0.595505618 | 0.404494382 | 0.5013 | 0.806890848 | 0.408928924 |
| X6 > 0.0896  | X61 > -0.5179  | X32 <= -1.3273 | 201  | 547  | 0.040304792 | 0.109116298 | 0.268716578 | 0.731283422 | 0.5013 | 1.458774032 | 0.408928924 |
| X6 > 0.0896  | X61 > -0.5179  | X32 > -1.3273  | 1162 | 754  | 0.233005815 | 0.150408937 | 0.606471816 | 0.393528184 | 0.5013 | 0.785015328 | 0.408928924 |

